{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9453632,"sourceType":"datasetVersion","datasetId":5746479}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport random\nimport numpy as np \nimport pandas as pd\nfrom tqdm import tqdm\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Constant","metadata":{}},{"cell_type":"code","source":"FILE_PATH= \"/kaggle/input/shop-lifter/Shop DataSet/non shop lifters/shop_lifter_n_0.mp4\" # Example to test functions\nvideo_path = '/kaggle/input/shop-lifter/Shop DataSet'  # Path to your dataset\nclass_0 = \"non shop lifters\"\nclass_1 = \"shop lifters\"\nout_path = \"/kaggle/working/aug_vid\"\nif not os.path.exists(out_path):\n    os.makedirs(out_path)\n# Parameters\n             # Number of frames to extract from each video\nheight, width = 90, 90        # Dimensions to resize each frame\nchannels = 3                  # Number of channels (RGB)\nbatch_size = 4          # Batch size for training\nepochs = 30                   # Number of training epochs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### List all Videos with labels ","metadata":{}},{"cell_type":"code","source":"all_videos = []\nfor label, class_name in enumerate(os.listdir(video_path)):\n    class_path = os.path.join(video_path, class_name)\n    if os.path.isdir(class_path):\n        for video_name in os.listdir(class_path):\n            video_file = os.path.join(class_path, video_name)\n            if video_file.endswith('.mp4'):  # Adjust for your video format\n                all_videos.append((video_file, label))\n\n\nprint(f\"Total videos: {len(all_videos)}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_videos [0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Video Augmentation","metadata":{}},{"cell_type":"code","source":"import imgaug.augmenters as iaa","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\nimport numpy as np\nimport random\n\ndef augment_video(frames):\n    \"\"\"Apply a random combination of augmentations to video frames using albumentations.\"\"\"\n    \n    # Define the individual augmentations\n    transform = A.Compose([\n        A.HorizontalFlip(p=0.5),  # Flip with a 50% chance\n        A.Rotate(limit=10, p=0.5),  # Randomly rotate between -10 and 10 degrees\n        A.RandomCrop(width=20, height=20, p=0.5),  # Random crop\n        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),  # Random brightness/contrast adjustment\n    ])\n\n    # Apply augmentations to each frame\n    frames_aug = [transform(image=frame)['image'] for frame in frames]\n\n    return frames_aug\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split into training and testing sets\ntrain_videos, test_videos = train_test_split(all_videos, test_size=0.2, stratify=[label for _, label in all_videos], random_state=42)\n\nprint(f\"Total videos: {len(all_videos)}\")\nprint(f\"Training videos: {len(train_videos)}\")\nprint(f\"Testing videos: {len(test_videos)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_frames(video_path):\n    \"\"\"Count the number of frames in a video file.\"\"\"\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        print(f\"Error opening video file {video_path}\")\n        return 0\n    \n    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    cap.release()\n    return frame_count","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def average_frames(video_dir):\n    \"\"\"Calculate the average number of frames for all videos in the specified directory.\"\"\"\n    video_files = [f for f in os.listdir(video_dir) if f.endswith('.mp4')]\n    total_frames = 0\n    video_count = 0\n    \n    for video_file in video_files:\n        video_path = os.path.join(video_dir, video_file)\n        frame_count = count_frames(video_path)\n        if frame_count > 0:  # Ensure valid frame count\n            total_frames += frame_count\n            video_count += 1\n\n    # Calculate average\n    average = total_frames / video_count if video_count > 0 else 0\n    return average\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Path to your dataset\navg_frames_0 = average_frames(os.path.join(video_path,class_0))\nprint(f\"Average number of frames per video in class 0: {avg_frames_0:.2f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Path to your dataset\navg_frames_1 = average_frames(os.path.join(video_path,class_1))\nprint(f\"Average number of frames per video in class 1: {avg_frames_1:.2f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Total_frames_average  = (avg_frames_0  + avg_frames_1) //2\nTotal_frames_average","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_frames = int(Total_frames_average * .8 )\n# num_frames = int(Total_frames_average)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names = []\nvideo_counts = []\nfor label, class_name in enumerate(os.listdir(video_path)):\n    class_path = os.path.join(video_path, class_name)\n    if os.path.isdir(class_path):  # Ensure it's a directory (class folder)\n        class_names.append(class_name)\n        video_counts.append(len(os.listdir(class_path)))\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label = []\nfor i, loops in enumerate(video_counts):\n    for j in range(loops):\n        label.append(i)\nprint(len(label))    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils import class_weight\n\nclass_weights = class_weight.compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(label),\n    y=label\n)\n\n# Convert to a dictionary\nclass_weight_dict = dict(enumerate(class_weights))\nprint(class_weight_dict) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime\ndef process_and_augment_videos(all_videos, out_path, num_frames=200, frame_step=15):\n    for i in range(0, len(all_videos), frame_step):\n        video, label = all_videos[i]\n        \n        if label == 1:\n            frames = []\n            cap = cv2.VideoCapture(video)\n            \n            if not cap.isOpened():\n                print(f\"Error opening video file {video}\")\n                continue\n\n            frame_count = 0\n            # Read frames from the video\n            while cap.isOpened() and frame_count < num_frames:\n                ret, frame = cap.read()\n                if not ret:\n                    break\n\n                # Resize frame to fixed dimensions if needed\n                frame = cv2.resize(frame, (width, height))\n                \n                # Append frame to the list\n                frames.append(frame)\n                frame_count += 1\n\n            cap.release()\n\n            # Convert list of frames to NumPy array for augmentation\n            video_np = np.array(frames)\n            \n            # Perform augmentation\n            augmented_frames = augment_video(video_np)\n\n            # Save augmented video\n            video_filename = f'{str(datetime.datetime.now())}.mp4'\n            video_output_path = os.path.join(out_path, video_filename)\n\n            # Define video writer with the same width and height as frames\n            out = cv2.VideoWriter(video_output_path, cv2.VideoWriter_fourcc(*'mp4v'), 20, (width, height))\n\n            # Write augmented frames to the output video file\n            for frame in augmented_frames:\n                out.write(frame)\n\n            out.release()\n\n            # Append new video path and label to `all_videos`\n            all_videos.append((video_output_path, label))\n\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"process_and_augment_videos(all_videos, out_path, num_frames=num_frames, frame_step=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Total videos: {len(all_videos)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split into training and testing sets\ntrain_videos, test_videos = train_test_split(all_videos, test_size=0.2, stratify=[label for _, label in all_videos], random_state=42)\n\nprint(f\"Total videos: {len(all_videos)}\")\nprint(f\"Training videos: {len(train_videos)}\")\nprint(f\"Testing videos: {len(test_videos)}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Video generator","metadata":{}},{"cell_type":"code","source":"def video_generator(videos, batch_size, num_frames, height, width, channels, class_weights ):\n    \"\"\"Generator function to yield video batches with class weights from pre-defined video list.\"\"\"\n    while True:  # Loop indefinitely\n        random.shuffle(videos)  # Shuffle the videos for each epoch\n\n        data = []\n        labels = []\n        weights = []\n\n        # Iterate through the shuffled list of videos\n        for video_file, label in videos:\n            frames = []\n            cap = cv2.VideoCapture(video_file)\n\n            if not cap.isOpened():\n                print(f\"Error opening video file {video_file}\")\n                continue\n\n            frame_count = 0\n            # Read frames from the video\n            while cap.isOpened() and frame_count < num_frames:\n                ret, frame = cap.read()\n                if not ret:\n                    break\n                # Resize frame and append to the frames list\n                frame = cv2.resize(frame, (width, height))\n                frame = frame.astype('float32') / 255.0\n                frames.append(frame)\n                frame_count += 1\n            \n            cap.release()\n\n            # If the video has fewer frames than required, pad with zeros\n            while len(frames) < num_frames:\n                frames.append(np.zeros((height, width, channels), dtype=np.uint8))\n\n            # Convert frames list to numpy array\n            frames = np.array(frames)\n          \n            \n            # Append the processed video, its label, and the corresponding weight\n            data.append(frames)\n            labels.append(label)\n            \n            \n           \n            #  weights.append(class_weights[label])  # Use class_weights\n\n            # Yield batch of videos when data list reaches batch_size\n            if len(data) == batch_size:\n#                 yield np.array(data), np.array(labels), np.array(weights)\n                yield np.array(data), np.array(labels)\n                data = []  # Reset data list for the next batch\n                labels = []  # Reset labels list for the next batch\n#                 weights = []  # Reset weights list for the next batch\n\n        # If there are leftover videos that don't fill a full batch, yield them\n        if len(data) > 0:\n#             yield np.array(data), np.array(labels), np.array(weights)\n            yield np.array(data), np.array(labels)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the generators\ngen = video_generator(train_videos, batch_size, num_frames, height, width, channels, class_weights )\ntest_gen = video_generator(test_videos, batch_size, num_frames, height, width, channels, class_weights)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for batch in gen:\n    videos , labels = batch\n    print(videos.shape)\n    print(labels)\n\n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Viualization","metadata":{}},{"cell_type":"code","source":"def plot_images(video_list):\n    \"\"\"Plot a list of 4 images in a 2x2 grid.\"\"\"\n    fig, axes = plt.subplots(2,4, figsize=(12, 6))  \n    axes = axes.flatten()  \n    \n    for i, video in enumerate(video_list):\n        axes[i].imshow(video[20,:,:,:] )\n        axes[i].axis('off')  \n\n    plt.tight_layout()\n    plt.show()\n\n\nplot_images(videos)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the results\nplt.figure(figsize=(10, 6))\nplt.bar(class_names, video_counts, color='skyblue')\nplt.xlabel('Class Name')\nplt.ylabel('Number of Videos')\nplt.title('Number of Videos per Class')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_videos[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count the total number of videos in the list\ntotal_videos = len(all_videos)\nprint(\"Total number of videos:\", total_videos)\n\n# Count the number of videos for each class\nclass_0_count = sum(1 for video in all_videos if video[1] == 0)\nclass_1_count = sum(1 for video in all_videos if video[1] == 1)\n\nprint(\"Number of class 0 videos:\", class_0_count)\nprint(\"Number of class 1 videos:\", class_1_count)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count the number of videos for each class\nclass_0_count = sum(1 for video in all_videos if video[1] == 0)\nclass_1_count = sum(1 for video in all_videos if video[1] == 1)\n\n\nclasses = ['Non Shop Lifters (Class 0)', 'Shop Lifters (Class 1)']\ncounts = [class_0_count, class_1_count]\n\n\n\nplt.figure(figsize=(10, 6))\nplt.bar(classes, counts, color='skyblue')\nplt.xlabel('Classes')\nplt.ylabel('Number of Videos')\nplt.title('Distribution of Videos by Class After Aug')\nplt.xticks(rotation=10)\nplt.grid(axis='y')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pretrained model","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import backend as K\n\n# Clear the session to release memory\nK.clear_session()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"steps_per_epoch = len(train_videos) // batch_size\nvalidation_steps = len(test_videos) // batch_size","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau,TensorBoard\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\nlr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\ncheckpoint = ModelCheckpoint('new_ShopLifter.keras',\n    monitor='val_loss', \n    mode='min',\n    save_best_only=True,\n    save_freq='epoch',\n    verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.get_logger().setLevel('ERROR')  \nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\ntf.get_logger().setLevel('INFO')  # Set to 'DEBUG' for more details\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the pretrained MobileNetV2 model\nbase_model = tf.keras.applications.MobileNetV2(\n    include_top=False, \n    weights='imagenet', \n    input_shape=(height, width, channels)\n)\n\n# Freeze the base model layers\nbase_model.trainable = False\n\n# Add your custom layers on top of the base model\nmodel = tf.keras.Sequential([\n    tf.keras.layers.TimeDistributed(base_model, input_shape=(num_frames, height, width, channels)),\n    tf.keras.layers.GlobalAveragePooling3D(),\n    tf.keras.layers.Dense(2, activation='softmax')  # Use softmax for binary classification\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\ntry:\n    hist = model.fit(\n        gen,\n        steps_per_epoch=steps_per_epoch,\n        epochs=epochs,\n        validation_data=test_gen,\n        validation_steps=validation_steps,\n        callbacks=[early_stopping, checkpoint, lr_scheduler],\n        verbose=1  # Verbose output to monitor training progress\n    )\nexcept Exception as e:\n    print(f\"Training error: {e}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluatation","metadata":{}},{"cell_type":"code","source":"# Plot the accuracy\nplt.plot(hist.history['accuracy'], label='Training Accuracy')\nplt.plot(hist.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the loss\nplt.plot(hist.history['loss'], label='Training Loss')\nplt.plot(hist.history['val_loss'], label='Validation Loss')\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\n# plt.ylim(0, 10)\nplt.legend(loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model\nmodel.save('shop_lifter_model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_gen = video_generator(test_videos, batch_size, num_frames, height, width, channels, class_weights)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.models.load_model(\"shop_lifter_model.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss, accuracy = model.evaluate(test_gen, steps=validation_steps)\nprint(f\"Loss: {loss}\")\nprint(f\"Accuracy: {accuracy}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the predicted class probabilities\ny_pred = model.predict(test_gen, steps=validation_steps)\n\n# Convert probabilities to class predictions (assuming binary classification)\ny_pred_classes = np.argmax(y_pred, axis=1)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_videos[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to load video frames\ntest_gen = video_generator(test_videos, batch_size, num_frames, height, width, channels, class_weights)\n\n# Parameters\nnum_frames = num_frames  \ntarget_size = (width, height)  \n\n# Prepare lists for true and predicted labels\nresults = []\n\n# Loop through the test videos to evaluate\nfor video_frames, true_label in test_gen:\n    if len(results) > 100:\n        break\n    if len(video_frames) == 0:\n        continue  \n\n    # Make predictions\n    predictions = model.predict(video_frames)\n    predicted_label = np.argmax(predictions, axis=1)  # Get the predicted classes for the batch\n\n    # Store the true and predicted labels for each video in the batch\n    for i in range(len(true_label)):\n        results.append((true_label[i], predicted_label[i]))\n\n# Convert results to a NumPy array for confusion matrix\nresults = np.array(results)\n\n# Extract true and predicted labels separately\ntrue_labels = results[:, 0]\npredicted_labels = results[:, 1]\n\n\n","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_labels.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_labels.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n# Generate confusion matrix\n# Optionally, generate a confusion matrix\nconf_matrix = confusion_matrix(true_labels, predicted_labels)\n\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Plot confusion matrix\nplt.figure(figsize=(6,6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Non Shoplifters', 'Shoplifters'], yticklabels=['Non Shoplifters', 'Shoplifters'])\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels, target_names=['Non Shoplifters', 'Shoplifters'])\nprint(report)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}